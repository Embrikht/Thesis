\chapter{Discussion\label{ch:discussion}}

This chapter analyzes the results of the website fingerprinting attacks on encrypted 802.11 frames and compares the performed attack to the trivial success probability. It addresses the implications and limitations of these findings.

The chapter begins with discussing the results of the website fingerprinting attack in \Cref{sec:accuracy-discussion} before examining the impact of aggregation in \Cref{sec:aggregation-discussion}. A comparison between the website fingerprinting attack and the trivial success probability is examined in \Cref{sec:trivadv-discussion}.

\clearpage 

\section{The accuracy of the website fingerprinting attack\label{sec:accuracy-discussion}}

The results show that the Gaussian näive Bayes classifier achieved a minimal accuracy of $95\%$ when distinguishing between two data types, when considering packet lengths with directionality. As the size of the privacy set increased, the accuracy decreased to a value between roughly $72\%$ to $92\%$, depending on the scenario, when distinguishing $30$ different data types. However, using packet lengths with directionality as features provided better accuracy than considering total trace time, upstream/downstream total bytes and number of packets, number of traffic bursts, and bytes in those bursts as the feature set. Despite the drop in accuracy with a larger privacy set, the accuracy obtained was still significantly better than selecting an element from the privacy set uniformly at random.

Previous research considered a fingerprinting attack on LTE/4G traffic \cite{kohls2019lost}, achieving a success rate of $92\%$ to $95\%$ in a controlled lab environment and around $90\%$ success rate in a real-world scenario. The `Alexa top 50 websites' list was used in these scenarios, i.e., their scenarios considered $k=50$. Additional research on the failure of website fingerprinting countermeasures illustrated a $80\%$ accuracy for $k=128$ \cite{DBLP:conf/sp/DyerCRS12}. In contrast, the accuracy achieved in this thesis was lower as the size of the privacy set increased when total trace time, upstream/downstream total bytes and number of packets, number of traffic bursts, and bytes in those bursts were among the features considered. This difference in accuracy is because of the smaller dataset and the simplicity of the classifier.

While the case of $k=2$ with $t=100$ trials indicates a real-world attacker's accuracy, the dataset used in this thesis is relatively limited compared to previous studies \cite{DBLP:conf/sp/DyerCRS12, DBLP:conf/ctrsa/GellertJLN22, DBLP:conf/pet/GongBKS12, DBLP:conf/pet/Hintz02, kohls2019lost, DBLP:conf/pet/MillerHJT14, DBLP:conf/ccnc/MuehlsteinZBKDD17, DBLP:journals/popets/WangG16}. This limitation arises from the data being collected over several days rather than several months, resulting in a less credible accuracy, especially as the size of the privacy set increases, leading to a decrease in the number of trials $t$. However, the accuracy still indicates the success rate an attacker can expect. While more data would yield a more precise accuracy, it is unlikely that the results would differ significantly. In addition, this thesis considers a maximum value of $k=30$ per trial, which is relatively low compared to a real-world scenario. For instance, the research in \cite{DBLP:conf/sp/DyerCRS12} considered $128$ and $775$ different data types, which is more realistic. Moreover, the classifiers used in this research were more complex, and the authors focused more on the accuracy achieved. Fine-tuning hyperparameters and using more advanced machine learning models capable of capturing intricate patterns would likely result in higher accuracy. This thesis uses the Gaussian näive Bayes with no hyperparameter tuning, requiring minimal expertise in classifiers. As a consequence, the accuracy achieved in this thesis should be interpreted as a lower bound, demonstrating the potential of classifiers.

The scenarios considered in this thesis differ from previous research \cite{DBLP:conf/sp/DyerCRS12, kohls2019lost}, making a direct comparison of accuracy challenging despite the observed traffic sharing similar characteristics. This thesis considers total trace time, upstream/downstream total bytes and number of packets, the number of traffic bursts, and bytes in those bursts as a single feature set, which was considered individually in earlier studies \cite{DBLP:conf/sp/DyerCRS12}. Additionally, the study in \cite{kohls2019lost} focused on LTE/4G traffic, which have fundamental differences compared to website fingerprinting of encrypted network traffic.

Similar to previous research on fingerprinting attacks on encrypted traffic \cite{DBLP:conf/sp/DyerCRS12, DBLP:conf/ctrsa/GellertJLN22, DBLP:conf/pet/GongBKS12, DBLP:conf/pet/Hintz02, DBLP:conf/pet/MillerHJT14, DBLP:conf/ccnc/MuehlsteinZBKDD17, DBLP:journals/popets/WangG16}, the results suggest that an attacker can execute a website fingerprinting attack on encrypted Wi-Fi traffic with a high success rate, particularly when using packet lengths with directionality as the feature set. As the size of the privacy set increases, the accuracy drops slower when packet lengths with directionality are considered. This suggests that with a larger dataset, the optimal strategy is considering packet lengths as the feature set.

Although there are limitations, the results indicate that the achieved accuracy aligns with findings from previous studies when focusing on the highest accuracy achieved. The results highlight the feasibility of performing a website fingerprinting attack on encrypted 802.11 frames, illustrating how information leakage undermines the security that encryption intuitively should provide. This emphasizes the importance of the underlying problem: while an encryption scheme may be cryptographically secure, it is not necessarily secure in practice if the encrypted network traffic leaks information through side channels.

Experimental results do not provide an exact accuracy measurement. They simulate a setting intended to reflect a real-world scenario. The computers in the experiments were physically connected, ensuring data quality with the client's background applications disabled. In a real-world scenario, attackers must distinguish traffic based on observed encrypted 802.11 frames, which is challenging. This issue is well known, where correctly distinguishing data is essential for obtaining quality data that attacks can exploit.

\section{Impact of aggregation\label{sec:aggregation-discussion}}

The results indicate that enabling aggregation led to a general reduction in the classifier's accuracy, although some anomalies were observed. However, this reduction was minimal and did not substantially reduce the accuracy. In the simple PDF fingerprinting scenario, the gap in classifier accuracy between A-MSDU enabled and disabled grew as the size of the privacy set increased. In the remaining scenarios, the reduction in accuracy remained relatively constant.

The impact of padding on encrypted network traffic has been previously explored in prior studies \cite{DBLP:conf/sp/DyerCRS12, DBLP:conf/pet/MillerHJT14}. One study \cite{DBLP:conf/pet/MillerHJT14} examined traffic analysis attacks on HTTPS traffic using classifiers. The study demonstrated that padding all packet sizes up to the nearest power of $2$ significantly reduced the attacker's success rate from $60\%$ to $22\%$ for one type of attack and $89\%$ to $59\%$ for another. The other study \cite{DBLP:conf/sp/DyerCRS12} compared various padding techniques and found that none of the nine countermeasures effectively prevented website fingerprinting attacks, given the scenario considered. Applying the best-performing countermeasure reduced the accuracy to around only $85\%$. The results from both studies indicate that padding affects the classifier's accuracy, even though they do not necessarily prevent website fingerprinting attacks. Anomalies in this thesis indicated increased accuracy when aggregation was enabled. These instances occurred when total trace time, upstream/downstream total bytes and number of packets, number of traffic bursts, and bytes in those bursts were the features considered. One reason for this is the impact of aggregation on traffic patterns.

By comparing \Cref{fig:web_amsdu_peekaboo,,fig:wiki_amsdu_peekaboo,,fig:domain_amsdu_peekaboo} with \Cref{fig:web_plain_peekaboo,,fig:wiki_plain_peekaboo,,fig:domain_plain_peekaboo}, the figures demonstrate how A-MSDU modifies specific traffic patterns of the encrypted Wi-Fi traffic, with the former showing A-MSDU enabled and the latter displaying A-MSDU disabled. While the change in traffic patterns result in reduced classifier accuracy, the results indicate that, in some instances, this action benefits the classifier. Since this effect is inconsistent across all cases, a possible conclusion is that the impact of aggregation depends on the characteristics of the different data types. If the data share similarities, aggregation might reveal distinguishing information, aiding the classifier. Conversely, the results suggest that aggregation alters the traffic patterns where A-MSDU clusters specific characteristics, making it more challenging for the classifier to differentiate.

While the results indicate that aggregation can assist the classifier in specific instances, the findings suggest that enabling A-MSDU tends to reduce the classifier's accuracy. Previous research \cite{DBLP:conf/ctrsa/GellertJLN22} focusing on length-hiding encryption in a theoretical context has shown that even a modest amount of padding can substantially reduce the adversary's advantage, with only a slight increase in bandwidth overhead. However, the results indicate that padding introduced by aggregation does not substantially lower the success rate in practical scenarios. This implies that relying solely on aggregation as a mitigation technique against website fingerprinting attacks is insufficient, as it only marginally reduces the accuracy of the classifier. A-MSDU should still be employed to reduce header overhead and increase bandwidth throughput. 

This thesis does not precisely measure aggregation's impact or explain the observed anomalies. While the data suggests that aggregation generally lowers classifier accuracy, the effect is inconsistent. Additionally, while the figures demonstrate changes in traffic patterns when aggregation is enabled, they do not explain the improved accuracy in certain situations, such as in the Wikipedia and domain fingerprinting scenarios. A possible hypothesis is that alternation in the traffic patterns may benefit the classifier under certain conditions. Further analysis would be required to provide an accurate explanation, as time constraints limited the ability to conduct a thorough investigation.

\section{Accuracy and the trivial success probability\label{sec:trivadv-discussion}}

For the simple PDF fingerprinting scenario, the trivial success probability is given by $\tfrac{|\mathcal{S}|}{900}$, which results in a trivial advantage of $\tfrac{|\mathcal{S}|-1}{899}$. Recall that the trivial advantage is a scaled value between $0$ and $1$ depending on the trivial success probability. The results show that there were $899$ different ciphertext lengths when no padding was applied, resulting in a trivial advantage of $0.9989 \approx 1$. As the size of $\ell$ increased, the trivial advantage decreased. However, the results indicate that a small amount of padding slightly reduced the trivial advantage. A significant amount of padding is needed to ensure no uniquely identifiable PDF files or all PDF files have the same length.

This calculation is based on previous research \cite{DBLP:conf/ctrsa/GellertJLN22}, which quantitatively measured the effectiveness of length-hiding encryption in concealing information about the message length. In one of their scenarios, they considered a simple website fingerprinting attack where a client visits a website consisting of many static HTML pages. In this case, the trivial success probability was $\tfrac{|\mathcal{S}|}{1620}$. Their results showed that without padding, the trivial advantage was $0.74$. However, when padding to a multiple of $80$, the advantage decreased to $0.0327$, with an overhead of only $1.158\%$. They concluded that a surprisingly small amount of padding may significantly reduce an adversary's advantage. This thesis suggests, in contrast, that while a small amount of padding reduces an attacker's trivial advantage, the reduction is much smaller compared to the results in previous research. Padding to a multiple of approximately $3797773$ in this thesis reduces the trivial advantage, similar to the effect of padding to a multiple of $80$ in the previous research. This difference is due to the type of data considered.

The previous research focused on a client visiting a website with many static HTML pages. These pages share a similar, plain text formatting, with references to external resources that need to be fetched when rendered on the client side. This thesis, however, considers PDF files, which may include text, images, graphics, and embedded elements, all of which affects their size. PDF files are typically much larger than static HTML pages, where each PDF file ranges from a few pages to many. This causes the size range of PDF files to be much broader than that of static HTML pages, leading to more unique file lengths in the case of PDF files. This reflects the trivial advantage of an adversary, where no padding is applied, resulting in $0.9989$ compared to $0.74$. This explains why the effectiveness of the padding is considerably reduced.

The findings indicate that an adversary can achieve high accuracy by only focusing on the message length in the simple PDF fingerprinting scenario. Due to the significant differences in file sizes, the results show that substantial padding is required to prevent leakage of message length through side channels. This comes at a cost of considerable overhead. However, the trivial advantage is notably greater than the accuracy of the classifier. While the achieved accuracy in both cases is similar for $\ell=1$ and $k=2$, the trivial advantage holds for all $900$ PDF files hosted on the web server. This results in a much higher trivial advantage compared to the success rate of a website fingerprinting attack when packet lengths with directionality are used as features. As the classifier's accuracy declines from approximately $99\%$ to $93\%$ for $k=30$, the results suggest that for $k=900$ is significantly worse than the trivial advantage in the theoretical model. These findings demonstrate that focusing solely on message length can provide insights into information leakage through side channels and how trivial success probability serves as a proof-of-concept. However, the comparison shows that the theoretical model does not accurately represent the success rate of a conducted website fingerprinting attack.

The results do not accurately measure of how well the trivial advantage reflects a real-world adversary. The proof-of-concept aims to demonstrate the leakage of message length through 802.11 frames in an idealized scenario, assuming a one-to-one correspondence between the length of PDF files and their corresponding encrypted 802.11 frames. This comparison can provide a clearer understanding of how theoretical measures relate to actual website fingerprinting attacks. However, as it stands, the results can only indicate an attacker's success probability, not an accurate measure.

\clearpage